kafka {
  map.input.topic = "local.map.input"
  map.output.topic = "local.map.output"
  event.input.topic = "local.event.input"
  event.output.topic = "local.event.output"
  string.input.topic = "local.string.input"
  string.output.topic = "local.string.output"
  producer.broker-servers = "localhost:9093"
  consumer.broker-servers = "localhost:9093"
  groupId = "pipeline-preprocessor-group"
  producer {
    max-request-size = 102400
    batch.size = 8192
    linger.ms = 1
  }
  output.system.event.topic = "flink.system.events"
  output.failed.topic = "flink.failed"
  event.duplicate.topic = "local.duplicate.output"
}

job {
  env = "local"
  statebackend {
    blob {
      storage {
        account = "blob.storage.account"
        container = "obsrv-container"
        checkpointing.dir = "flink-jobs"
      }
    }
  }
}

kafka.output.metrics.topic = "pipeline_metrics"
task {
  checkpointing.interval = 60000
  checkpointing.pause.between.seconds = 30000
  restart-strategy.attempts = 1
  restart-strategy.delay = 10000
  parallelism = 1
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

redis.connection.timeout = 30000

redis {
  host = 127.0.0.1
  port = 6340
  database {
    duplicationstore.id = 12
    key.expiry.seconds = 3600
  }
}

redis-meta {
  host = localhost
  port = 6340
}

postgres {
    host = localhost
    port = 5432
    maxConnection = 2
    user = "postgres"
    password = "postgres"
}

dataset.type = "master-dataset"
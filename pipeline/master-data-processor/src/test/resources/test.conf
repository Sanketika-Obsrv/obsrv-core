include "base-test.conf"

job {
  env = "flink"
}

kafka {

  input.topic = ${job.env}".masterdata.ingest"
  output.raw.topic = ${job.env}".masterdata.raw"
  output.extractor.duplicate.topic = ${job.env}".masterdata.failed"
  output.failed.topic = ${job.env}".masterdata.failed"
  output.batch.failed.topic = ${job.env}".masterdata.failed"
  event.max.size = "1048576" # Max is only 1MB
  output.invalid.topic = ${job.env}".masterdata.failed"
  output.unique.topic = ${job.env}".masterdata.unique"
  output.duplicate.topic = ${job.env}".masterdata.failed"
  output.denorm.topic = ${job.env}".masterdata.denorm"
  output.transform.topic = ${job.env}".masterdata.transform"
  output.transform.failed.topic = ${job.env}".masterdata.transform.failed"
  stats.topic = ${job.env}".masterdata.stats"
  groupId = ${job.env}"-masterdata-pipeline-group"
  producer {
    max-request-size = 5242880
  }
}

task {
  window.time.in.seconds = 2
  window.count = 2
  window.shards = 1400
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

redis {
  port = 6340
  database {
    extractor.duplication.store.id = 1
    preprocessor.duplication.store.id = 2
    key.expiry.seconds = 3600
  }
}

dataset.type = "master-dataset"

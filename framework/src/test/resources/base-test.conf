kafka {
  producer.broker-servers = "localhost:9093"
  consumer.broker-servers = "localhost:9093"
  map.input.topic = "local.map.input"
  map.output.topic = "local.map.output"
  event.input.topic = "local.event.input"
  event.output.topic = "local.event.output"
  event.duplicate.topic = "local.duplicate.output"
  string.input.topic = "local.string.input"
  string.output.topic = "local.string.output"
  groupId = "test-consumer-group"
  auto.offset.reset = "earliest"
  producer {
    max-request-size = 102400
    batch.size = 8192
    linger.ms = 1
    compression = "snappy"
  }
  output.system.event.topic = "flink.system.events"
  output.failed.topic = "flink.failed"
}

job {
  env = "local"
  enable.distributed.checkpointing = false
  statebackend {
    blob {
      storage {
        account = "blob.storage.account"
        container = "obsrv-container"
        checkpointing.dir = "flink-jobs"
      }
    }
    base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
  }
}

task {
  checkpointing.interval = 60000
  checkpointing.pause.between.seconds = 30000
  restart-strategy.attempts = 1
  restart-strategy.delay = 10000
  parallelism = 1
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

redis {
  host = 127.0.0.1
  port = 6340
  database {
    duplication.store.id = 12
    key.expiry.seconds = 3600
  }
  connection.timeout = 30000
  database.duplicationstore.id = 1
}

redis-meta {
  host = localhost
  port = 6340
}

postgres {
    host = localhost
    port = 5432
    maxConnections = 2
    user = "postgres"
    password = "postgres"
    database="postgres"
}
otel.collector.endpoint="http://0.0.0.0:4317"
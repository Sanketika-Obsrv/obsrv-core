include "baseconfig.conf"

kafka {
  input.topic = ${job.env}".hudi.connector.in"
  output.topic = ${job.env}".hudi.connector.out"
  output.invalid.topic = ${job.env}".failed"
  event.max.size = "1048576" # Max is only 1MB
  groupId = ${job.env}"-hudi-writer-group"
  producer {
    max-request-size = 5242880
  }
}

task {
  checkpointing.compressed = true
  checkpointing.interval = 30000
  checkpointing.pause.between.seconds = 30000
  restart-strategy.attempts = 3
  restart-strategy.delay = 30000 # in milli-seconds
  parallelism = 1
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}

hudi {
  hms {
    enabled = true
    uri = "thrift://localhost:9083"
    database {
      name = "obsrv"
      username = "postgres"
      password = "postgres"
    }
  }
  table {
    type = "MERGE_ON_READ"
    base.path = "s3a://obsrv"
  }
  compaction.enabled = true
  metadata.enabled = true
  write {
    tasks = 1
    task.max.memory = 512
    compaction.max.memory = 100
  }
  write.batch.size = 16
  compaction.tasks = 1
  delta.commits = 2
  delta.seconds = 600
  compression.codec = "snappy"
  index.type = "BLOOM"
}
kafka {
  map.input.topic = "local.map.input"
  map.output.topic = "local.map.output"
  event.input.topic = "local.event.input"
  event.output.topic = "local.event.output"
  string.input.topic = "local.string.input"
  string.output.topic = "local.string.output"
  producer.broker-servers = "localhost:9093"
  consumer.broker-servers = "localhost:9093"
  groupId = "pipeline-preprocessor-group"
  auto.offset.reset = "earliest"
  producer {
    max-request-size = 102400
    batch.size = 8192
    linger.ms = 1
    compression = "snappy"
  }
}

job {
  env = "local"
  enable.distributed.checkpointing = false
  statebackend {
    blob {
      storage {
        account = "blob.storage.account"
        container = "obsrv-container"
        checkpointing.dir = "flink-jobs"
      }
    }
    base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
  }
}

kafka.output.metrics.topic = "pipeline_metrics"
task {
  parallelism = 1
  consumer.parallelism = 1
  checkpointing.interval = 60000
  checkpointing.pause.between.seconds = 30000
  metrics.window.size = 100 # 3 min
  restart-strategy.attempts = 1 # retry once
  restart-strategy.delay = 1000 # in milli-seconds
}

redis.connection.timeout = 30000

redis {
  host = 127.0.0.1
  port = 6340
  database {
    duplicationstore.id = 12
    key.expiry.seconds = 3600
  }
}

redis-meta {
  host = localhost
  port = 6340
}

postgres {
    host = localhost
    port = 5432
    maxConnection = 2
    user = "postgres"
    password = "postgres"
}
